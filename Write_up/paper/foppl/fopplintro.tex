% !TEX root = ../main.tex
\section{FOPPL}
\label{sec:foppl}
The FOPPL\citep{woodgroup2017} that we use throughout this paper has a syntax based on the \mintinline{clojure}{clojure} \citep{Hickey2008} programming language and so one can implement conditional statements, like \mintinline{clojure}{if} and primitive operations such as \mintinline{clojure}{+,-,/,*}. However, unlike \mintinline{clojure}{clojure} there is no recursion and so you cannot unroll a loop an infinite number of times. This is something that you could do, for instance, in a HOPPL. Despite this seemingly reduced functionality, a FOPPL demonstrates the wide ranging utility of probabilistic programming languages and it is flexible enough to allow one to implement complicated finite graphical models. This means that we can even write neural networks within a FOPPL \citep{woodgroup2017}. When we design a program in a FOPPL, a FOPPL outputs a finite directed graph that provides us with the joint distribution, from which it extracts the posterior of the model.\\
To construct a FOPPL program we have a set of syntactic statements at our disposal, such as \mintinline{clojure}{observe}, \mintinline{clojure}{sample, def, foppl-query}, \mintinline{clojure}{defn} and \mintinline{clojure}{let}. That allow us to \mintinline{clojure}{def}ine programs, and \mintinline{clojure}{foppl-query}ies within programs,  by \mintinline{clojure}{let}ting you \mintinline{clojure}{defn}e functions and assign variables to \mintinline{clojure}{observe}d values and obtain probabilistic variables through \mintinline{clojure}{sample}ing from distribution objects. In order to compile a FOPPL program to Python, we must preserve the structure of our directed graph, like the ones presented in section \ref{sec:exampprog}.  We map the edges, vertices and nodes accordingly so that the structure of the joint is preserved. We also take into account which values are our latent parameters and which values are observed, as within Pytorch, latent variables must be defined in a special way. This is because in order to perform HMC on continuous systems, the gradients of the latent parameters are needed and so we must predefine them as  \mintinline{python}{torch.autograd.variable.Variable()} objects, with the following flag \mintinline{python}{requires_grad= True}.\\
To ensure that those variables, other constants and objects remain unique within our compilation, we introduce a unique naming procedure to ensure that our graphical model structure is preserved.  To do this we set all latent parameters equal to \mintinline{python}{'x' + str(<unique_key>)} and for each latent parameter in which some primitive operation is performed with the original variable to create a new assignment, the new assignment will have an \mintinline{python}{'x'} label and a string of numbers that is unique, but will increase by one for each operation performed. All priors, likelihoods and posterior distributions are given the labels \mintinline{python}{'p' + str(<unique_key>) }, all constants and observed values are given the label \mintinline{python}{'c' + str(<unique_key>) } and if the constant is an observed datum, then the compiler sets  \mintinline{python}{'y'+str(<unique_key>)='c'+str(<unique_key>)}. As Pytorch does not support distribution objects, we had to define our own class of distribution objects, which directly map, for example, a \mintinline{clojure}{normal} object in a FOPPL, to a \mintinline{python}{normal} object in Python. For these distribution objects the label \mintinline{python}{'d' + str(<unique_key>)} is assigned. See the supplementary material, section \ref{sec:supmat} for examples of the compiled output.